# ğŸ§  Unsupervised Learning Algorithms

Unsupervised learning is a branch of machine learning where the model learns **patterns and structures from unlabeled data**. Unlike supervised learning, we do not provide explicit input-output pairs â€” the algorithm discovers hidden structures, groupings, or representations by itself.

---

## ğŸ“š Key Categories

### 1. **Clustering**
Groups data points into similar clusters based on feature similarity.

- **k-Means Clustering** â€“ partitions data into *k* groups by minimizing intra-cluster variance.
- **Hierarchical Clustering** â€“ builds a tree (dendrogram) of clusters, can be agglomerative (bottom-up) or divisive (top-down).
- **DBSCAN (Density-Based Spatial Clustering)** â€“ groups points that are closely packed; identifies noise/outliers.
- **Gaussian Mixture Models (GMM)** â€“ probabilistic clustering using a mixture of Gaussian distributions.

---

### 2. **Dimensionality Reduction**
Reduces the number of features while preserving essential structure.

- **PCA (Principal Component Analysis)** â€“ projects data into directions of maximum variance.
- **t-SNE (t-Distributed Stochastic Neighbor Embedding)** â€“ creates a 2D/3D embedding for visualization.
- **UMAP (Uniform Manifold Approximation and Projection)** â€“ preserves local and global structure better than t-SNE.
- **Autoencoders** â€“ neural networks that compress data into a lower-dimensional latent representation.

---

### 3. **Association Rule Learning**
Discovers interesting relationships between variables.

- **Apriori Algorithm** â€“ identifies frequent itemsets and association rules.
- **FP-Growth** â€“ a faster alternative to Apriori for frequent pattern mining.

---

### 4. **Anomaly / Novelty Detection**
Identifies data points that deviate significantly from the norm.

- **Isolation Forest** â€“ isolates anomalies by random partitioning.
- **One-Class SVM** â€“ learns a decision boundary around normal data.
- **Local Outlier Factor (LOF)** â€“ detects density-based anomalies.

---

## ğŸ¯ Common Use Cases
- **Customer Segmentation** (clustering)
- **Market Basket Analysis** (association rules)
- **Dimensionality Reduction** for visualization or preprocessing
- **Anomaly Detection** in fraud detection, cybersecurity, or manufacturing
- **Representation Learning** with autoencoders

---

## ğŸ› ï¸ Quick Example (Clustering with k-Means)

```python
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

# Generate synthetic data
X, _ = make_blobs(n_samples=300, centers=4, random_state=42)

# Fit k-Means
kmeans = KMeans(n_clusters=4, random_state=42)
labels = kmeans.fit_predict(X)

# Plot results
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap="viridis")
plt.scatter(kmeans.cluster_centers_[:, 0],
            kmeans.cluster_centers_[:, 1],
            c="red", marker="X", s=200, label="Centroids")
plt.title("k-Means Clustering Example")
plt.legend()
plt.show()

