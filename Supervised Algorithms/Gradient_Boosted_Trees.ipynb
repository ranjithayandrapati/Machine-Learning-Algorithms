{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install scikit-learn matplotlib pandas numpy\n",
        "\n",
        "# --- Imports ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # no-op in recent sklearn, safe to keep\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
        "from sklearn.inspection import permutation_importance\n",
        "from scipy.stats import randint, loguniform\n",
        "import joblib\n"
      ],
      "metadata": {
        "id": "dAHW344rXLWU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0364958d-e38d-4186-beb3-5eb668ca5f59"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/experimental/enable_hist_gradient_boosting.py:19: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1) Load a sample dataset (Iris) ---\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "target_names = iris.target_names"
      ],
      "metadata": {
        "id": "NHNKOHWPbzMH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2) Train/validation split ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")"
      ],
      "metadata": {
        "id": "ztf-SkFyXWUn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3) Baseline HistGradientBoosting ---\n",
        "hgb_base = HistGradientBoostingClassifier(\n",
        "    learning_rate=0.1,\n",
        "    max_depth=None,          # unlimited depth; consider 3-10 for regularization\n",
        "    max_leaf_nodes=31,       # typical default\n",
        "    max_iter=300,            # like n_estimators\n",
        "    min_samples_leaf=20,\n",
        "    l2_regularization=0.0,\n",
        "    early_stopping=True,\n",
        "    validation_fraction=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "hgb_base.fit(X_train, y_train)\n",
        "y_pred_base = hgb_base.predict(X_test)\n",
        "\n",
        "print(\"Baseline Accuracy:\", accuracy_score(y_test, y_pred_base))\n",
        "print(\"\\nBaseline Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_base, target_names=target_names))"
      ],
      "metadata": {
        "id": "fyiPmpjnXjQs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24d1634b-4d56-4bea-891e-396e587852d3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Accuracy: 0.9210526315789473\n",
            "\n",
            "Baseline Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        12\n",
            "  versicolor       0.86      0.92      0.89        13\n",
            "   virginica       0.92      0.85      0.88        13\n",
            "\n",
            "    accuracy                           0.92        38\n",
            "   macro avg       0.92      0.92      0.92        38\n",
            "weighted avg       0.92      0.92      0.92        38\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4) Hyperparameter tuning (RandomizedSearchCV) ---\n",
        "# Uses log-uniform for LR & L2 (common for boosting); keeps cv=3 for speed.\n",
        "param_dist = {\n",
        "    \"learning_rate\": loguniform(1e-3, 3e-1),\n",
        "    \"max_depth\": [None, 3, 5, 7, 10],\n",
        "    \"max_leaf_nodes\": randint(15, 65),\n",
        "    \"min_samples_leaf\": randint(1, 30),\n",
        "    \"l2_regularization\": loguniform(1e-10, 1e-2),\n",
        "    \"max_iter\": randint(150, 501),   # boosting rounds\n",
        "}\n",
        "hgb = HistGradientBoostingClassifier(\n",
        "    early_stopping=True,\n",
        "    validation_fraction=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "rand = RandomizedSearchCV(\n",
        "    estimator=hgb,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=40,               # increase for more thorough search\n",
        "    scoring=\"accuracy\",\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=0,\n",
        ")\n",
        "rand.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nBest Params:\", rand.best_params_)\n",
        "print(\"Best CV Accuracy:\", rand.best_score_)"
      ],
      "metadata": {
        "id": "eXsuMdn6XlkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5) Evaluate best model ---\n",
        "best_model = rand.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"\\nTest Accuracy (Best Model):\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report (Best Model):\")\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "# Confusion matrix\n",
        "disp = ConfusionMatrixDisplay.from_predictions(\n",
        "    y_test, y_pred, display_labels=target_names, xticks_rotation=45\n",
        ")\n",
        "plt.title(\"Confusion Matrix - HistGradientBoosting (Best Model)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sjyrwSU2ZZ_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6) Feature importance (permutation) ---\n",
        "# Works even when model doesn't expose tree-based importances cleanly.\n",
        "perm = permutation_importance(\n",
        "    best_model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1\n",
        ")\n",
        "imp = perm.importances_mean\n",
        "indices = np.argsort(imp)\n",
        "\n",
        "plt.figure(figsize=(6, 4.5))\n",
        "plt.barh(range(len(indices)), imp[indices])\n",
        "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
        "plt.xlabel(\"Mean Permutation Importance (Î” accuracy)\")\n",
        "plt.title(\"Permutation Feature Importances\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- 7) Save the trained model ---\n",
        "joblib.dump(best_model, \"hgb_best_model.joblib\")\n",
        "print(\"\\nSaved best model to hgb_best_model.joblib\")"
      ],
      "metadata": {
        "id": "_N_FOXcVdRT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 8) Predict on a new sample (match feature order!) ---\n",
        "# For Iris: [sepal length, sepal width, petal length, petal width]\n",
        "sample = np.array([[5.4, 3.9, 1.7, 0.4]])\n",
        "pred = best_model.predict(sample)[0]\n",
        "proba = best_model.predict_proba(sample)[0]\n",
        "print(f\"\\nNew sample predicted class: {target_names[pred]}\")\n",
        "print(\"Class probabilities:\", dict(zip(target_names, np.round(proba, 3))))"
      ],
      "metadata": {
        "id": "Pcp6XwbTdSam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1oY0KwZ1xew7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}